import random

import gensim
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import nltk
import numpy as np
import pandas as pd
import plotly.express as px
import pyLDAvis.gensim_models
import regex
import seaborn as sns
import streamlit as st
import streamlit.components.v1 as components
from gensim import corpora
from gensim.models import CoherenceModel
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from umap import UMAP
from wordcloud import WordCloud

# ê¸°ë³¸ ì„¤ì • ê°’ ì •ì˜
DEFAULT_HIGHLIGHT_PROBABILITY_MINIMUM = 0.001
DEFAULT_NUM_TOPICS = 6

# NLTKì˜ ë¶ˆìš©ì–´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download("stopwords")

# ë°ì´í„°ì…‹ ì •ë³´ ì‚¬ì „ ì •ì˜
# DATASETS = {
#     'Five Years of Elon Musk Tweets': {
#         'path': './data/elonmusk.csv.zip',
#         'column': 'tweet',
#         'url': 'https://www.kaggle.com/vidyapb/elon-musk-tweets-2015-to-2020',
#         'description': (
#             'twint ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœê·¼ 5ë…„ê°„ Elon Muskì˜ íŠ¸ìœ—ì„ ìŠ¤í¬ë©í–ˆìŠµë‹ˆë‹¤. '
#             'ì´ ë°ì´í„°ì…‹ì„ í†µí•´ ê³µì¸ ì¸ì‚¬ê°€ ì†Œì…œ ë¯¸ë””ì–´ í”Œë«í¼ì—ì„œ ì¼ë°˜ì¸ì—ê²Œ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ '
#             'ì‚´í´ë³´ê³ ì í–ˆìŠµë‹ˆë‹¤. Teslaê°€ ì£¼ë¡œ ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ íŠ¸ìœ—í•˜ëŠ”ì§€, '
#             'Elon Muskì˜ íŠ¸ìœ—ì´ Tesla ì£¼ì‹ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë“±ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ë…¸íŠ¸ë¶ì„ ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤.'
#         )
#     },
#     'Airline Tweets': {
#         'path': './data/Tweets.csv.zip',
#         'column': 'text',
#         'url': 'https://www.kaggle.com/crowdflower/twitter-airline-sentiment',
#         'description': (
#             'ê° ì£¼ìš” ë¯¸êµ­ í•­ê³µì‚¬ì˜ ë¬¸ì œì ì— ëŒ€í•œ ê°ì„± ë¶„ì„ ì‘ì—…ì…ë‹ˆë‹¤. '
#             '2015ë…„ 2ì›”ì— ìŠ¤í¬ë©ëœ íŠ¸ìœ„í„° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì—¬ìë“¤ì€ ë¨¼ì € ê¸ì •ì , ë¶€ì •ì , ì¤‘ë¦½ì ì¸ íŠ¸ìœ—ì„ ë¶„ë¥˜í•œ í›„, '
#             '"ëŠ¦ì€ ë¹„í–‰" ë˜ëŠ” "ë¬´ë¡€í•œ ì„œë¹„ìŠ¤"ì™€ ê°™ì€ ë¶€ì •ì  ì´ìœ ë¥¼ ë¶„ë¥˜í•˜ë„ë¡ ìš”ì²­ë°›ì•˜ìŠµë‹ˆë‹¤.'
#         )
#     }
# }

# LDA ëª¨ë¸ ì˜µì…˜ì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
def lda_options():
    return {
        'num_topics': st.number_input(
            'í† í”½ ìˆ˜',
            min_value=1,
            value=9,
            help='í•™ìŠµ ì½”í¼ìŠ¤ì—ì„œ ì¶”ì¶œí•  ì ì¬ í† í”½ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'chunksize': st.number_input(
            'ì²­í¬ í¬ê¸°',
            min_value=1,
            value=2000,
            help='ê° í•™ìŠµ ì²­í¬ì— ì‚¬ìš©í•  ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'passes': st.number_input(
            'íŒ¨ìŠ¤ ìˆ˜',
            min_value=1,
            value=1,
            help='í•™ìŠµ ì¤‘ ì½”í¼ìŠ¤ë¥¼ í†µê³¼í•˜ëŠ” íŒ¨ìŠ¤ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'update_every': st.number_input(
            'ì—…ë°ì´íŠ¸ ì£¼ê¸°',
            min_value=1,
            value=1,
            help='ê° ì—…ë°ì´íŠ¸ë§ˆë‹¤ ë°˜ë³µí•  ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë°°ì¹˜ í•™ìŠµì„ ì›í•  ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •í•˜ê³ , ë°˜ë³µ í•™ìŠµì„ ì›í•  ê²½ìš° 1 ì´ìƒìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'alpha': st.selectbox(
            'ğ›¼',
            ('symmetric', 'asymmetric', 'auto'),
            help='ë¬¸ì„œ-í† í”½ ë¶„í¬ì— ëŒ€í•œ ì‚¬ì „ê°’(priori belief)ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'eta': st.selectbox(
            'ğœ‚',
            (None, 'symmetric', 'auto'),
            help='í† í”½-ë‹¨ì–´ ë¶„í¬ì— ëŒ€í•œ ì‚¬ì „ê°’(priori belief)ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'decay': st.number_input(
            'ğœ…',
            min_value=0.5,
            max_value=1.0,
            value=0.5,
            help='ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ ê²€ì‚¬í•  ë•Œ ì´ì „ ëŒë‹¤ ê°’ì„ ì–¼ë§ˆë‚˜ ìŠì–´ë²„ë¦´ì§€ ê²°ì •í•˜ëŠ” (0.5, 1] ì‚¬ì´ì˜ ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'offset': st.number_input(
            'ğœ_0',
            value=1.0,
            help='ì²˜ìŒ ëª‡ ë²ˆì˜ ë°˜ë³µì—ì„œ í•™ìŠµ ì†ë„ë¥¼ ì–¼ë§ˆë‚˜ ëŠ¦ì¶œì§€ë¥¼ ì œì–´í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.'
        ),
        'eval_every': st.number_input(
            'í‰ê°€ ì£¼ê¸°',
            min_value=1,
            value=10,
            help='ì–¼ë§ˆë‚˜ ìì£¼ í¼í”Œë ‰ì„œí‹°ë¥¼ ë¡œê·¸ë¡œ í‰ê°€í• ì§€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'iterations': st.number_input(
            'ë°˜ë³µ íšŸìˆ˜',
            min_value=1,
            value=50,
            help='ì½”í¼ìŠ¤ì˜ í† í”½ ë¶„í¬ë¥¼ ì¶”ë¡ í•  ë•Œ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'gamma_threshold': st.number_input(
            'ğ›¾',
            min_value=0.0,
            value=0.001,
            help='ê°ë§ˆ íŒŒë¼ë¯¸í„° ê°’ì˜ ìµœì†Œ ë³€í™”ëŸ‰ì„ ì„¤ì •í•˜ì—¬ ë°˜ë³µì„ ê³„ì†í• ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.'
        ),
        'minimum_probability': st.number_input(
            'ìµœì†Œ í™•ë¥ ',
            min_value=0.0,
            max_value=1.0,
            value=0.01,
            help='ì´ ì„ê³„ê°’ë³´ë‹¤ ë‚®ì€ í™•ë¥ ì„ ê°€ì§„ í† í”½ì€ í•„í„°ë§ë©ë‹ˆë‹¤.'
        ),
        'minimum_phi_value': st.number_input(
            'ğœ‘',
            min_value=0.0,
            value=0.01,
            help='per_word_topicsê°€ Trueì¼ ê²½ìš°, ìš©ì–´ í™•ë¥ ì˜ í•˜í•œì„ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'per_word_topics': st.checkbox(
            'ë‹¨ì–´ë³„ í† í”½',
            help='Trueë¡œ ì„¤ì •í•˜ë©´ ëª¨ë¸ì´ ê° ë‹¨ì–´ì— ëŒ€í•œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ í† í”½ ëª©ë¡ê³¼ ê·¸ phi ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.'
        )
    }

# NMF ëª¨ë¸ ì˜µì…˜ì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
def nmf_options():
    return {
        'num_topics': st.number_input(
            'í† í”½ ìˆ˜',
            min_value=1,
            value=9,
            help='ì¶”ì¶œí•  í† í”½ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'chunksize': st.number_input(
            'ì²­í¬ í¬ê¸°',
            min_value=1,
            value=2000,
            help='ê° í•™ìŠµ ì²­í¬ì— ì‚¬ìš©í•  ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'passes': st.number_input(
            'íŒ¨ìŠ¤ ìˆ˜',
            min_value=1,
            value=1,
            help='í•™ìŠµ ì½”í¼ìŠ¤ë¥¼ í†µê³¼í•˜ëŠ” ì „ì²´ íŒ¨ìŠ¤ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'kappa': st.number_input(
            'ğœ…',
            min_value=0.0,
            value=1.0,
            help='ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìŠ¤í… í¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'minimum_probability': st.number_input(
            'ìµœì†Œ í™•ë¥ ',
            min_value=0.0,
            max_value=1.0,
            value=0.01,
            help=(
                'normalizeê°€ Trueì¼ ê²½ìš°, í™•ë¥ ì´ ì‘ì€ í† í”½ì€ í•„í„°ë§ë©ë‹ˆë‹¤. '
                'normalizeê°€ Falseì¼ ê²½ìš°, ì¸ìˆ˜ê°€ ì‘ì€ íŒ©í„°ê°€ í•„í„°ë§ë©ë‹ˆë‹¤. '
                'Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ 1e-8 ê°’ì´ ì‚¬ìš©ë˜ì–´ 0ì„ ë°©ì§€í•©ë‹ˆë‹¤.'
            )
        ),
        'w_max_iter': st.number_input(
            'W ìµœëŒ€ ë°˜ë³µ',
            min_value=1,
            value=200,
            help='ê° ë°°ì¹˜ì—ì„œ Wë¥¼ í•™ìŠµí•  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'w_stop_condition': st.number_input(
            'W ì •ì§€ ì¡°ê±´',
            min_value=0.0,
            value=0.0001,
            help='ì˜¤ì°¨ ì°¨ì´ê°€ ì´ ê°’ë³´ë‹¤ ì‘ì•„ì§€ë©´ í˜„ì¬ ë°°ì¹˜ì˜ W í•™ìŠµì„ ì¤‘ì§€í•©ë‹ˆë‹¤.'
        ),
        'h_max_iter': st.number_input(
            'H ìµœëŒ€ ë°˜ë³µ',
            min_value=1,
            value=50,
            help='ê° ë°°ì¹˜ì—ì„œ Hë¥¼ í•™ìŠµí•  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'h_stop_condition': st.number_input(
            'H ì •ì§€ ì¡°ê±´',
            min_value=0.0,
            value=0.001,
            help='ì˜¤ì°¨ ì°¨ì´ê°€ ì´ ê°’ë³´ë‹¤ ì‘ì•„ì§€ë©´ í˜„ì¬ ë°°ì¹˜ì˜ H í•™ìŠµì„ ì¤‘ì§€í•©ë‹ˆë‹¤.'
        ),
        'eval_every': st.number_input(
            'í‰ê°€ ì£¼ê¸°',
            min_value=1,
            value=10,
            help='v - Whì˜ l2 ë…¸ë¦„ì„ ê³„ì‚°í•  ë°°ì¹˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        ),
        'normalize': st.selectbox(
            'ì •ê·œí™”',
            (True, False, None),
            help='ê²°ê³¼ë¥¼ ì •ê·œí™”í• ì§€ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
        )
    }

# ì§€ì›ë˜ëŠ” ëª¨ë¸ ì •ë³´ ì‚¬ì „ ì •ì˜
MODELS = {
    'Latent Dirichlet Allocation': {
        'options': lda_options,
        'class': gensim.models.LdaModel,
        'help': 'https://radimrehurek.com/gensim/models/ldamodel.html'
    },
    'Non-Negative Matrix Factorization': {
        'options': nmf_options,
        'class': gensim.models.Nmf,
        'help': 'https://radimrehurek.com/gensim/models/nmf.html'
    }
}

# Matplotlibì—ì„œ ì‚¬ìš©í•  ìƒ‰ìƒ ëª©ë¡ ìƒì„±
COLORS = [color for color in mcolors.XKCD_COLORS.values()]

# ì›Œë“œí´ë¼ìš°ë“œì— ì‚¬ìš©í•  í°íŠ¸ ê²½ë¡œ
WORDCLOUD_FONT_PATH = r'./data/Proxima Nova Regular.otf'

# ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì •ì˜
EMAIL_REGEX_STR = r'\S*@\S*'
MENTION_REGEX_STR = r'@\S*'
HASHTAG_REGEX_STR = r'#\S+'
URL_REGEX_STR = r'((http|https)\:\/\/)?[a-zA-Z0-9\.\/\?\:@\-_=#]+\.([a-zA-Z]){2,6}([a-zA-Z0-9\.\&\/\?\:@\-_=#])*'

# ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì„ íƒëœ ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
@st.cache_data()
def generate_texts_df(selected_dataset: str):
    dataset = DATASETS[selected_dataset]
    return pd.read_csv(f'{dataset["path"]}')

# ë¬¸ì„œì—ì„œ ë¶ˆí•„ìš”í•œ ìš”ì†Œë¥¼ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜
@st.cache_data()
def denoise_docs(texts_df: pd.DataFrame, text_column: str):
    texts = texts_df[text_column].values.tolist()
    # ì´ë©”ì¼, ë©˜ì…˜, í•´ì‹œíƒœê·¸, URL ì œê±°ë¥¼ ìœ„í•œ ì •ê·œí‘œí˜„ì‹ ì»´íŒŒì¼
    remove_regex = regex.compile(f'({EMAIL_REGEX_STR}|{MENTION_REGEX_STR}|{HASHTAG_REGEX_STR}|{URL_REGEX_STR})')
    texts = [regex.sub(remove_regex, '', text) for text in texts]
    # ê°„ë‹¨í•œ ì „ì²˜ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±°
    docs = [
        [w for w in simple_preprocess(doc, deacc=True) if w not in stopwords.words('english')]
        for doc in texts
    ]
    return docs

# ë¹…ê·¸ë¨(2-ê·¸ë¨)ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
@st.cache_data()
def create_bigrams(docs):
    bigram_phrases = gensim.models.Phrases(docs)
    bigram_phraser = gensim.models.phrases.Phraser(bigram_phrases)
    docs = [bigram_phraser[doc] for doc in docs]
    return docs

# íŠ¸ë¼ì´ê·¸ë¨(3-ê·¸ë¨)ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
@st.cache_data()
def create_trigrams(docs):
    bigram_phrases = gensim.models.Phrases(docs)
    bigram_phraser = gensim.models.phrases.Phraser(bigram_phrases)
    trigram_phrases = gensim.models.Phrases(bigram_phrases[docs])
    trigram_phraser = gensim.models.phrases.Phraser(trigram_phrases)
    docs = [trigram_phraser[bigram_phraser[doc]] for doc in docs]
    return docs

# ë¬¸ì„œ ìƒì„± í•¨ìˆ˜ë¡œ, ì „ì²˜ë¦¬ ë° n-ê·¸ë¨ ìƒì„±ì„ í¬í•¨
@st.cache_data()
def generate_docs(texts_df: pd.DataFrame, text_column: str, ngrams: str = None):
    docs = denoise_docs(texts_df, text_column)
    if ngrams == 'bigrams':
        docs = create_bigrams(docs)
    if ngrams == 'trigrams':
        docs = create_trigrams(docs)
    return docs

# ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
@st.cache_data()
def generate_wordcloud(docs, collocations: bool = False):
    wordcloud_text = (' '.join(' '.join(doc) for doc in docs))
    wordcloud = WordCloud(
        font_path=WORDCLOUD_FONT_PATH,
        width=700,
        height=600,
        background_color='white',
        collocations=collocations
    ).generate(wordcloud_text)
    return wordcloud

# í›ˆë ¨ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜
@st.cache_data()
def prepare_training_data(docs):
    id2word = corpora.Dictionary(docs)
    corpus = [id2word.doc2bow(doc) for doc in docs]
    return id2word, corpus

# ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” í•¨ìˆ˜
@st.cache_data()
def train_model(docs, base_model, **kwargs):
    id2word, corpus = prepare_training_data(docs)
    model = base_model(corpus=corpus, id2word=id2word, **kwargs)
    return id2word, corpus, model

# ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜
def clear_session_state():
    for key in (
        'model_kwargs',
        'id2word',
        'corpus',
        'model',
        'previous_perplexity',
        'previous_coherence_model_value'
    ):
        if key in st.session_state:
            del st.session_state[key]

# í¼í”Œë ‰ì„œí‹°ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
def calculate_perplexity(model, corpus):
    return np.exp2(-model.log_perplexity(corpus))

# ì½”íˆëŸ°ìŠ¤ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
def calculate_coherence(model, corpus, coherence):
    coherence_model = CoherenceModel(model=model, corpus=corpus, coherence=coherence)
    return coherence_model.get_coherence()

# ë°°ê²½ìƒ‰ì— ë”°ë¼ í…ìŠ¤íŠ¸ ìƒ‰ìƒì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
@st.cache_data()
def white_or_black_text(background_color):
    # ë°°ê²½ìƒ‰ì— ë”°ë¼ ê²€ì • ë˜ëŠ” í°ìƒ‰ í…ìŠ¤íŠ¸ ì„ íƒ
    red = int(background_color[1:3], 16)
    green = int(background_color[3:5], 16)
    blue = int(background_color[5:], 16)
    return 'black' if (red * 0.299 + green * 0.587 + blue * 0.114) > 186 else 'white'

# í¼í”Œë ‰ì„œí‹° ì„¹ì…˜ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def perplexity_section():
    with st.spinner('í¼í”Œë ‰ì„œí‹° ê³„ì‚° ì¤‘...'):
        perplexity = calculate_perplexity(st.session_state.model, st.session_state.corpus)
    key = 'previous_perplexity'
    delta = f'{perplexity - st.session_state[key]:.4}' if key in st.session_state else None
    st.metric(
        label='í¼í”Œë ‰ì„œí‹°',
        value=f'{perplexity:.4f}',
        delta=delta,
        delta_color='inverse'
    )
    st.markdown('ì°¸ê³ : https://en.wikipedia.org/wiki/Perplexity')
    st.latex(r'Perplexity = \exp\left(-\frac{\sum_d \log(p(w_d|\Phi, \alpha))}{N}\right)')

# ì½”íˆëŸ°ìŠ¤ ì„¹ì…˜ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def coherence_section():
    with st.spinner('ì½”íˆëŸ°ìŠ¤ ì ìˆ˜ ê³„ì‚° ì¤‘...'):
        coherence = calculate_coherence(st.session_state.model, st.session_state.corpus, 'u_mass')
    key = 'previous_coherence_model_value'
    delta = f'{coherence - st.session_state[key]:.4f}' if key in st.session_state else None
    st.metric(
        label='ì½”íˆëŸ°ìŠ¤ ì ìˆ˜',
        value=f'{coherence:.4f}',
        delta=delta
    )
    st.session_state[key] = coherence
    st.markdown('ì°¸ê³ : http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf')
    st.latex(
        r'C_{UMass} = \frac{2}{N \cdot (N - 1)}\sum_{i=2}^N\sum_{j=1}^{i-1}\log\frac{P(w_i, w_j) + \epsilon}{P(w_j)}'
    )

# ì €ì°¨ì› íˆ¬ì˜ì„ í›ˆë ¨í•˜ëŠ” í•¨ìˆ˜
@st.cache_data()
def train_projection(projection, n_components, df):
    if projection == 'PCA':
        projection_model = PCA(n_components=n_components)
    elif projection == 'T-SNE':
        projection_model = TSNE(n_components=n_components)
    elif projection == 'UMAP':
        projection_model = UMAP(n_components=n_components)
    else:
        raise ValueError(f'ì•Œ ìˆ˜ ì—†ëŠ” íˆ¬ì˜ ë°©ì‹: {projection}')
    return projection_model.fit_transform(df)

# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
if __name__ == '__main__':
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title='ì£¼ì œ ëª¨ë¸ë§',
        page_icon='./data/favicon.png',
        layout='wide'
    )

    # ì‚¬ì´ë“œë°”ì˜ ì „ì²˜ë¦¬ ì˜µì…˜ í¼
    preprocessing_options = st.sidebar.form('preprocessing-options')
    with preprocessing_options:
        st.header('ì „ì²˜ë¦¬ ì˜µì…˜')
        ngrams = st.selectbox(
            'N-ê·¸ë¨',
            [None, 'bigrams', 'trigrams'],
            help='ë‹¨ì–´ì˜ ê²°í•© ì •ë„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì˜ˆ: ë°”ì´ê·¸ë¨, íŠ¸ë¼ì´ê·¸ë¨'
        )
        st.form_submit_button('ì „ì²˜ë¦¬ ì‹¤í–‰')

    # ì‚¬ì´ë“œë°”ì˜ ì‹œê°í™” ì˜µì…˜ í¼
    visualization_options = st.sidebar.form('visualization-options')
    with visualization_options:
        st.header('ì‹œê°í™” ì˜µì…˜')
        collocations = st.checkbox(
            'ì›Œë“œí´ë¼ìš°ë“œ ì½œë¡œì¼€ì´ì…˜ í™œì„±í™”',
            help='ì›Œë“œí´ë¼ìš°ë“œì—ì„œ êµ¬ë¬¸ì„ í‘œì‹œí•  ìˆ˜ ìˆë„ë¡ ì½œë¡œì¼€ì´ì…˜ì„ í™œì„±í™”í•©ë‹ˆë‹¤.'
        )
        highlight_probability_minimum = st.select_slider(
            'í•˜ì´ë¼ì´íŠ¸ í™•ë¥  ìµœì†Œê°’',
            options=[10 ** exponent for exponent in range(-10, 1)],
            value=DEFAULT_HIGHLIGHT_PROBABILITY_MINIMUM,
            help='_í† í”½ í•˜ì´ë¼ì´íŠ¸ ë¬¸ì¥_ ì‹œê°í™”ì—ì„œ ë‹¨ì–´ë¥¼ ìƒ‰ìƒìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸í•˜ê¸° ìœ„í•œ ìµœì†Œ í† í”½ í™•ë¥ ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
        )
        st.form_submit_button('ì ìš©')

    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì œëª© ë° ì„¤ëª…
    st.title('í† í”½ ëª¨ë¸ë§')
    # st.header('ì£¼ì œ ëª¨ë¸ë§ì´ë€?')
    # with st.expander('íˆì–´ë¡œ ì´ë¯¸ì§€'):
    #     st.image('./data/is-this-a-topic-modeling.jpg', caption='ì•„ë‹ˆìš” ... ì•„ë‹™ë‹ˆë‹¤ ...', use_column_width=True)
    # st.markdown(
    #     'ì£¼ì œ ëª¨ë¸ë§ì€ ë‹¤ì–‘í•œ í†µê³„ì  í•™ìŠµ ë°©ë²•ì„ í¬í•¨í•˜ëŠ” í¬ê´„ì ì¸ ìš©ì–´ì…ë‹ˆë‹¤. '
    #     'ì´ ë°©ë²•ë“¤ì€ ë¬¸ì„œë¥¼ ì¼ë ¨ì˜ ì£¼ì œë¡œ, ê·¸ ì£¼ì œë¥¼ ì¼ë ¨ì˜ ë‹¨ì–´ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. '
    #     'ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì€ ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹(LDA)ê³¼ ë¹„ìŒìˆ˜ í–‰ë ¬ ë¶„í•´(NMF)ì…ë‹ˆë‹¤. '
    #     'ì¶”ê°€ì ì¸ í•œì •ì–´ ì—†ì´ ì‚¬ìš©ë  ë•Œ, ì´ ì ‘ê·¼ ë°©ì‹ì€ ì¼ë°˜ì ìœ¼ë¡œ ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ ê°„ì£¼ë˜ì§€ë§Œ, '
    #     'ì¤€ì§€ë„ ë° ì§€ë„ í•™ìŠµ ë³€í˜•ë„ ì¡´ì¬í•©ë‹ˆë‹¤.'
    # )

    # # ì¶”ê°€ ì„¸ë¶€ ì •ë³´ í™•ì¥
    # with st.expander('ì¶”ê°€ ì„¸ë¶€ ì •ë³´'):
    #     st.markdown('ëª©í‘œëŠ” í–‰ë ¬ ë¶„í•´ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')
    #     st.image('./data/mf.png', use_column_width=True)
    #     st.markdown('ì´ ë¶„í•´ëŠ” ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì§ì ‘ íŠ¹ì„±í™”í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.')
    #     st.markdown('LDAì™€ NMFì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” '
    #                 'https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation ë° '
    #                 'https://en.wikipedia.org/wiki/Non-negative_matrix_factorization ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.')

    # ë°ì´í„°ì…‹ ì„¹ì…˜
    st.header('ë°ì´í„°ì…‹')
    # # st.markdown('ì„¤ëª…ì„ ìœ„í•´ ë¯¸ë¦¬ ë¡œë“œëœ ëª‡ ê°€ì§€ ì‘ì€ ì˜ˆì œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.')
    # selected_dataset = st.selectbox(
    #     'ë°ì´í„°ì…‹',
    #     [None, *sorted(list(DATASETS.keys()))],
    #     on_change=clear_session_state
    # )
    # if not selected_dataset:
    #     st.write('ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì—¬ ê³„ì†í•˜ì„¸ìš” ...')
    #     st.stop()

    # # ë°ì´í„°ì…‹ ì„¤ëª… í™•ì¥
    # with st.expander('ë°ì´í„°ì…‹ ì„¤ëª…'):
    #     st.markdown(DATASETS[selected_dataset]['description'])
    #     st.markdown(DATASETS[selected_dataset]['url'])

    # íŒŒì¼ ì—…ë¡œë” ì¶”ê°€
    st.header('íŒŒì¼ ì—…ë¡œë“œ')
    uploaded_files = st.file_uploader(
        "docx, pdf, txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['docx', 'pdf', 'txt'],
        accept_multiple_files=True
    )

    if not uploaded_files:
        st.warning('ì²˜ë¦¬í•  íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.')
        st.stop()

    # ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    def extract_text(file):
        if file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            # docx íŒŒì¼ ì²˜ë¦¬
            doc = Document(file)
            return '\n'.join([para.text for para in doc.paragraphs])
        elif file.type == "application/pdf":
            # pdf íŒŒì¼ ì²˜ë¦¬
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page in reader.pages:
                text += page.extract_text() + '\n'
            return text
        elif file.type == "text/plain":
            # txt íŒŒì¼ ì²˜ë¦¬
            return file.getvalue().decode("utf-8")
        else:
            return ""
        
    # ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë°ì´í„°í”„ë ˆì„ ìƒì„±
    texts = []
    for uploaded_file in uploaded_files:
        text = extract_text(uploaded_file)
        if text:
            texts.append({
                'íŒŒì¼ëª…': uploaded_file.name,
                'í…ìŠ¤íŠ¸': text
            })

    if not texts:
        st.error('ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        st.stop()

    texts_df = pd.DataFrame(texts)


    # ì—…ë¡œë“œëœ ë°ì´í„°ì˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì§€ì •
    text_column = 'í…ìŠ¤íŠ¸'
    docs = generate_docs(texts_df, text_column, ngrams=ngrams)

    # ìƒ˜í”Œ ë¬¸ì„œ í™•ì¥
    with st.expander('ìƒ˜í”Œ ë¬¸ì„œ'):
        sample_texts = texts_df[text_column].sample(5).values.tolist()
        for index, text in enumerate(sample_texts):
            st.markdown(f'**{index + 1}**: _{text}_')

    # ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™” í™•ì¥
    with st.expander('ë¹ˆë„ ê¸°ë°˜ ì½”í¼ìŠ¤ ì›Œë“œí´ë¼ìš°ë“œ'):
        wc = generate_wordcloud(docs)
        st.image(wc.to_image(), caption='ë°ì´í„°ì…‹ ì›Œë“œí´ë¼ìš°ë“œ (í† í”½ ëª¨ë¸ë§ ì•„ë‹˜)', use_column_width=True)
        st.markdown('ë¬¸ì„œ ì „ì²˜ë¦¬ í›„ ë‚¨ì€ ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤.')

    # ë¬¸ì„œ ë‹¨ì–´ ìˆ˜ ë¶„í¬ ì‹œê°í™” í™•ì¥
    with st.expander('ë¬¸ì„œ ë‹¨ì–´ ìˆ˜ ë¶„í¬'):
        len_docs = [len(doc) for doc in docs]
        df_len_docs = pd.DataFrame(len_docs, columns=['ë¬¸ì„œë‹¹ ë‹¨ì–´ ìˆ˜'])
        
        # Plotly Expressë¥¼ ì‚¬ìš©í•œ ì¸í„°ë™í‹°ë¸Œ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
        fig = px.histogram(
            df_len_docs,
            x='ë¬¸ì„œë‹¹ ë‹¨ì–´ ìˆ˜',
            nbins=50,  # ì›í•˜ëŠ” ë¹ˆì˜ ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥
            labels={'ë¬¸ì„œë‹¹ ë‹¨ì–´ ìˆ˜': 'ë¬¸ì„œë‹¹ ë‹¨ì–´ ìˆ˜', 'count': 'ë¹ˆë„'},
            title='ë¬¸ì„œ ë‹¨ì–´ ìˆ˜ ë¶„í¬',
            hover_data={'ë¬¸ì„œë‹¹ ë‹¨ì–´ ìˆ˜': True, 'count': True},
            opacity=0.75,
            color_discrete_sequence=['#636EFA']
        )
        
        # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸ (ì˜µì…˜)
        fig.update_layout(
            xaxis_title='Words in document',
            yaxis_title='Count',
            bargap=0.1,  # ë§‰ëŒ€ ì‚¬ì´ ê°„ê²©
            template='plotly_white'  # í°ìƒ‰ ë°°ê²½ í…œí”Œë¦¿
        )
        
        # Plotly íˆìŠ¤í† ê·¸ë¨ì„ Streamlitì— í‘œì‹œ
        st.plotly_chart(fig, use_container_width=True)


    # ëª¨ë¸ ì„ íƒ ì„¹ì…˜
    model_key = st.sidebar.selectbox(
        'ëª¨ë¸',
        [None, *list(MODELS.keys())],
        on_change=clear_session_state
    )
    model_options = st.sidebar.form('model-options')
    if not model_key:
        with st.sidebar:
            st.write('ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ê³„ì†í•˜ì„¸ìš” ...')
        st.stop()
    with model_options:
        st.header('ëª¨ë¸ ì˜µì…˜')
        model_kwargs = MODELS[model_key]['options']()
        st.session_state['model_kwargs'] = model_kwargs
        train_model_clicked = st.form_submit_button('ëª¨ë¸ í•™ìŠµ')

    # ëª¨ë¸ í•™ìŠµ ë²„íŠ¼ í´ë¦­ ì‹œ
    if train_model_clicked:
        with st.spinner('ëª¨ë¸ í•™ìŠµ ì¤‘...'):
            id2word, corpus, model = train_model(
                docs,
                MODELS[model_key]['class'],
                **st.session_state.model_kwargs
            )
        st.session_state.id2word = id2word
        st.session_state.corpus = corpus
        st.session_state.model = model

    # ëª¨ë¸ì´ ì„¸ì…˜ ìƒíƒœì— ì—†ëŠ” ê²½ìš° ì¤‘ì§€
    if 'model' not in st.session_state:
        st.stop()

    # ëª¨ë¸ ì •ë³´ ì„¹ì…˜
    st.header('ëª¨ë¸')
    st.write(type(st.session_state.model).__name__)
    st.write(st.session_state.model_kwargs)

    # ëª¨ë¸ ê²°ê³¼ ì„¹ì…˜
    st.header('ëª¨ë¸ ê²°ê³¼')

    # í† í”½ ì¶”ì¶œ
    topics = st.session_state.model.show_topics(
        formatted=False,
        num_words=50,
        num_topics=st.session_state.model_kwargs['num_topics'],
        log=False
    )
    with st.expander('í† í”½ ë‹¨ì–´ ê°€ì¤‘ì¹˜ ìš”ì•½'):
        topic_summaries = {}
        for topic in topics:
            topic_index = topic[0]
            topic_word_weights = topic[1]
            # ìƒìœ„ 10ê°œì˜ ë‹¨ì–´ì™€ ê°€ì¤‘ì¹˜ë¥¼ ìš”ì•½
            topic_summaries[topic_index] = ' + '.join(
                f'{weight:.3f} * {word}' for word, weight in topic_word_weights[:10]
            )
        for topic_index, topic_summary in topic_summaries.items():
            st.markdown(f'**í† í”½ {topic_index}**: _{topic_summary}_')

    # ëœë¤ ìƒ‰ìƒ ìƒ˜í”Œë§
    colors = random.sample(COLORS, k=model_kwargs['num_topics'])
    with st.expander('ìƒìœ„ N í† í”½ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ'):
        cols = st.columns(3)
        for index, topic in enumerate(topics):
            wc = WordCloud(
                font_path=WORDCLOUD_FONT_PATH,
                width=700,
                height=600,
                background_color='white',
                collocations=collocations,
                prefer_horizontal=1.0,
                color_func=lambda *args, **kwargs: colors[index]
            )
            with cols[index % 3]:
                wc.generate_from_frequencies(dict(topic[1]))
                st.image(wc.to_image(), caption=f'í† í”½ #{index}', use_column_width=True)

    # í† í”½ í•˜ì´ë¼ì´íŠ¸ ë¬¸ì¥ ì‹œê°í™” ì„¹ì…˜
    with st.expander('í† í”½ í•˜ì´ë¼ì´íŠ¸ ë¬¸ì¥'):
        sample = texts_df.sample(10)
        for index, row in sample.iterrows():
            html_elements = []
            for token in row[text_column].split():
                if st.session_state.id2word.token2id.get(token) is None:
                    # ë‹¨ì–´ê°€ ì‚¬ì „ì— ì—†ìœ¼ë©´ ì·¨ì†Œì„  ì ìš©
                    html_elements.append(f'<span style="text-decoration:line-through;">{token}</span>')
                else:
                    term_topics = st.session_state.model.get_term_topics(token, minimum_probability=0)
                    topic_probabilities = [term_topic[1] for term_topic in term_topics]
                    max_topic_probability = max(topic_probabilities) if topic_probabilities else 0
                    if max_topic_probability < highlight_probability_minimum:
                        # í™•ë¥ ì´ ìµœì†Œê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¼ë°˜ í…ìŠ¤íŠ¸
                        html_elements.append(token)
                    else:
                        # ìµœëŒ€ í™•ë¥ ì„ ê°€ì§„ í† í”½ì˜ ìƒ‰ìƒìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸
                        max_topic_index = topic_probabilities.index(max_topic_probability)
                        max_topic = term_topics[max_topic_index]
                        background_color = colors[max_topic[0]]
                        color = white_or_black_text(background_color)
                        html_elements.append(
                            f'<span style="background-color: {background_color}; color: {color}; opacity: 0.5;">{token}</span>'
                        )
            st.markdown(f'ë¬¸ì„œ #{index}: {" ".join(html_elements)}', unsafe_allow_html=True)

    # ëª¨ë¸ ë©”íŠ¸ë¦­ ì„¹ì…˜
    has_log_perplexity = hasattr(st.session_state.model, 'log_perplexity')
    with st.expander('ë©”íŠ¸ë¦­'):
        if has_log_perplexity:
            left_column, right_column = st.columns(2)
            with left_column:
                perplexity_section()
            with right_column:
                coherence_section()
        else:
            coherence_section()

    # ì €ì°¨ì› íˆ¬ì˜ ì‹œê°í™” ì„¹ì…˜
    with st.expander('ì €ì°¨ì› íˆ¬ì˜'):
        with st.form('projections-form'):
            left_column, right_column = st.columns(2)
            projection = left_column.selectbox(
                'íˆ¬ì˜ ë°©ì‹',
                ['PCA', 'T-SNE', 'UMAP'],
                help='ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ì— íˆ¬ì˜í•˜ëŠ” ë°©ì‹ì„ ì„ íƒí•©ë‹ˆë‹¤.'
            )
            plot_type = right_column.selectbox(
                'í”Œë¡¯ ìœ í˜•',
                ['2D', '3D'],
                help='2ì°¨ì› ë˜ëŠ” 3ì°¨ì› í”Œë¡¯ì„ ì„ íƒí•©ë‹ˆë‹¤.'
            )
            n_components = 3  # ê¸°ë³¸ì ìœ¼ë¡œ 3ì°¨ì› íˆ¬ì˜
            columns = [f'proj{i}' for i in range(1, 4)]
            generate_projection_clicked = st.form_submit_button('íˆ¬ì˜ ìƒì„±')

        if generate_projection_clicked:
            topic_weights = []
            for index, topic_weight in enumerate(st.session_state.model[st.session_state.corpus]):
                weight_vector = [0] * int(st.session_state.model_kwargs['num_topics'])
                for topic, weight in topic_weight:
                    weight_vector[topic] = weight
                topic_weights.append(weight_vector)
            df = pd.DataFrame(topic_weights)
            dominant_topic = df.idxmax(axis='columns').astype('string')
            dominant_topic_percentage = df.max(axis='columns')
            df = df.assign(
                dominant_topic=dominant_topic,
                dominant_topic_percentage=dominant_topic_percentage,
                text=texts_df[text_column]
            )
            with st.spinner('íˆ¬ì˜ í•™ìŠµ ì¤‘...'):
                projections = train_projection(
                    projection,
                    n_components,
                    df.drop(columns=['dominant_topic', 'dominant_topic_percentage', 'text']).add_prefix('topic_')
                )
            data = pd.concat([df, pd.DataFrame(projections, columns=columns)], axis=1)

            # Plotly ì˜µì…˜ ì„¤ì •
            px_options = {
                'color': 'dominant_topic',
                'size': 'dominant_topic_percentage',
                'hover_data': ['dominant_topic', 'dominant_topic_percentage', 'text']
            }
            if plot_type == '2D':
                fig = px.scatter(data, x='proj1', y='proj2', **px_options)
                st.plotly_chart(fig)
                fig = px.scatter(data, x='proj1', y='proj3', **px_options)
                st.plotly_chart(fig)
                fig = px.scatter(data, x='proj2', y='proj3', **px_options)
                st.plotly_chart(fig)
            elif plot_type == '3D':
                fig = px.scatter_3d(data, x='proj1', y='proj2', z='proj3', **px_options)
                st.plotly_chart(fig)

    # pyLDAvis ì‹œê°í™” ë²„íŠ¼ ë° ì„¹ì…˜
    if hasattr(st.session_state.model, 'inference'):  # gensim NmfëŠ” 'inference' ì†ì„±ì´ ì—†ì–´ pyLDAvis ì‹¤íŒ¨
        if st.button('pyLDAvis ìƒì„±'):
            with st.spinner('pyLDAvis ì‹œê°í™” ìƒì„± ì¤‘...'):
                py_lda_vis_data = pyLDAvis.gensim_models.prepare(
                    st.session_state.model,
                    st.session_state.corpus,
                    st.session_state.id2word
                )
                py_lda_vis_html = pyLDAvis.prepared_data_to_html(py_lda_vis_data)
            with st.expander('pyLDAvis', expanded=True):
                st.markdown(
                    'pyLDAvisëŠ” í•™ìŠµëœ ì£¼ì œ ëª¨ë¸ì˜ ì£¼ì œë¥¼ í•´ì„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ëŒ€í™”í˜• ì›¹ ê¸°ë°˜ ì‹œê°í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. '
                    'ì´ íŒ¨í‚¤ì§€ëŠ” í•™ìŠµëœ LDA ì£¼ì œ ëª¨ë¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.'
                )
                st.markdown('https://github.com/bmabey/pyLDAvis')
                components.html(py_lda_vis_html, width=1300, height=800)
