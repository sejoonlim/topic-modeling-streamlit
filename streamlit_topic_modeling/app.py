import os
import random
import regex
import numpy as np
import pandas as pd
import streamlit as st
import gensim
import matplotlib.colors as mcolors
import nltk
import plotly.express as px
import pyLDAvis.gensim_models
from docx import Document  # docx íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import PyPDF2  # pdf íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from gensim import corpora
from gensim.models import CoherenceModel, Phrases, LdaModel, Nmf
from gensim.models.phrases import Phraser
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
from wordcloud import WordCloud
from nltk.stem import WordNetLemmatizer
from io import BytesIO
import re  # íŠ¹ìˆ˜ ë¬¸ì ì œê±°ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ë³µí•©ì–´ ì‚¬ì „ ì •ì˜
REPLACEMENTS = {
    "cyber security": "cybersecurity",
}

# ê¸°ë³¸ ì„¤ì • ê°’ ì •ì˜
DEFAULT_HIGHLIGHT_PROBABILITY_MINIMUM = 0.001
DEFAULT_NUM_TOPICS = 6

# NLTKì˜ ë¶ˆìš©ì–´ ë° WordNet ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("omw-1.4")  # WordNet Lemmatizerë¥¼ ìœ„í•œ ë¦¬ì†ŒìŠ¤

# ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì •ì˜
EMAIL_REGEX_STR = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
MENTION_REGEX_STR = r'@\w+'
HASHTAG_REGEX_STR = r'#\w+'
URL_REGEX_STR = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

# ì›Œë“œí´ë¼ìš°ë“œì— ì‚¬ìš©í•  í°íŠ¸ ê²½ë¡œ
WORDCLOUD_FONT_PATH = os.path.join('.', 'data', 'Proxima Nova Regular.otf')

# ì§€ì›ë˜ëŠ” ëª¨ë¸ ì •ë³´ ì‚¬ì „ ì •ì˜
def lda_options():
    with st.sidebar.form('lda-options'):
        st.header('LDA ì˜µì…˜')
        return {
            'num_topics': st.number_input(
                'í† í”½ ìˆ˜',
                min_value=1,
                value=9,
                help='í•™ìŠµ ì½”í¼ìŠ¤ì—ì„œ ì¶”ì¶œí•  ì ì¬ í† í”½ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'chunksize': st.number_input(
                'ì²­í¬ í¬ê¸°',
                min_value=1,
                value=2000,
                help='ê° í•™ìŠµ ì²­í¬ì— ì‚¬ìš©í•  ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'passes': st.number_input(
                'íŒ¨ìŠ¤ ìˆ˜',
                min_value=1,
                value=1,
                help='í•™ìŠµ ì¤‘ ì½”í¼ìŠ¤ë¥¼ í†µê³¼í•˜ëŠ” íŒ¨ìŠ¤ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'update_every': st.number_input(
                'ì—…ë°ì´íŠ¸ ì£¼ê¸°',
                min_value=1,
                value=1,
                help='ê° ì—…ë°ì´íŠ¸ë§ˆë‹¤ ë°˜ë³µí•  ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë°°ì¹˜ í•™ìŠµì„ ì›í•  ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •í•˜ê³ , ë°˜ë³µ í•™ìŠµì„ ì›í•  ê²½ìš° 1 ì´ìƒìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'alpha': st.selectbox(
                'ğ›¼',
                ('symmetric', 'asymmetric', 'auto'),
                help='ë¬¸ì„œ-í† í”½ ë¶„í¬ì— ëŒ€í•œ ì‚¬ì „ê°’(priori belief)ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'eta': st.selectbox(
                'ğœ‚',
                (None, 'symmetric', 'auto'),
                help='í† í”½-ë‹¨ì–´ ë¶„í¬ì— ëŒ€í•œ ì‚¬ì „ê°’(priori belief)ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'decay': st.number_input(
                'ğœ…',
                min_value=0.5,
                max_value=1.0,
                value=0.5,
                help='ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ ê²€ì‚¬í•  ë•Œ ì´ì „ ëŒë‹¤ ê°’ì„ ì–¼ë§ˆë‚˜ ìŠì–´ë²„ë¦´ì§€ ê²°ì •í•˜ëŠ” (0.5, 1] ì‚¬ì´ì˜ ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'offset': st.number_input(
                'ğœ_0',
                value=1.0,
                help='ì²˜ìŒ ëª‡ ë²ˆì˜ ë°˜ë³µì—ì„œ í•™ìŠµ ì†ë„ë¥¼ ì–¼ë§ˆë‚˜ ëŠ¦ì¶œì§€ë¥¼ ì œì–´í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.'
            ),
            'eval_every': st.number_input(
                'í‰ê°€ ì£¼ê¸°',
                min_value=1,
                value=10,
                help='ì–¼ë§ˆë‚˜ ìì£¼ í¼í”Œë ‰ì„œí‹°ë¥¼ ë¡œê·¸ë¡œ í‰ê°€í• ì§€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'iterations': st.number_input(
                'ë°˜ë³µ íšŸìˆ˜',
                min_value=1,
                value=50,
                help='ì½”í¼ìŠ¤ì˜ í† í”½ ë¶„í¬ë¥¼ ì¶”ë¡ í•  ë•Œ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'gamma_threshold': st.number_input(
                'ğ›¾',
                min_value=0.0,
                value=0.001,
                help='ê°ë§ˆ íŒŒë¼ë¯¸í„° ê°’ì˜ ìµœì†Œ ë³€í™”ëŸ‰ì„ ì„¤ì •í•˜ì—¬ ë°˜ë³µì„ ê³„ì†í• ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.'
            ),
            'minimum_probability': st.number_input(
                'ìµœì†Œ í™•ë¥ ',
                min_value=0.0,
                max_value=1.0,
                value=0.01,
                help='ì´ ì„ê³„ê°’ë³´ë‹¤ ë‚®ì€ í™•ë¥ ì„ ê°€ì§„ í† í”½ì€ í•„í„°ë§ë©ë‹ˆë‹¤.'
            ),
            'minimum_phi_value': st.number_input(
                'ğœ‘',
                min_value=0.0,
                value=0.01,
                help='per_word_topicsê°€ Trueì¼ ê²½ìš°, ìš©ì–´ í™•ë¥ ì˜ í•˜í•œì„ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'per_word_topics': st.checkbox(
                'ë‹¨ì–´ë³„ í† í”½',
                help='Trueë¡œ ì„¤ì •í•˜ë©´ ëª¨ë¸ì´ ê° ë‹¨ì–´ì— ëŒ€í•œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ í† í”½ ëª©ë¡ê³¼ ê·¸ phi ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.'
            ),
            'submit': st.form_submit_button('ì ìš©')
        }

def nmf_options():
    with st.sidebar.form('nmf-options'):
        st.header('NMF ì˜µì…˜')
        return {
            'num_topics': st.number_input(
                'í† í”½ ìˆ˜',
                min_value=1,
                value=9,
                help='ì¶”ì¶œí•  í† í”½ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'chunksize': st.number_input(
                'ì²­í¬ í¬ê¸°',
                min_value=1,
                value=2000,
                help='ê° í•™ìŠµ ì²­í¬ì— ì‚¬ìš©í•  ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'passes': st.number_input(
                'íŒ¨ìŠ¤ ìˆ˜',
                min_value=1,
                value=1,
                help='í•™ìŠµ ì½”í¼ìŠ¤ë¥¼ í†µê³¼í•˜ëŠ” ì „ì²´ íŒ¨ìŠ¤ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'kappa': st.number_input(
                'ğœ…',
                min_value=0.0,
                value=1.0,
                help='ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìŠ¤í… í¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'minimum_probability': st.number_input(
                'ìµœì†Œ í™•ë¥ ',
                min_value=0.0,
                max_value=1.0,
                value=0.01,
                help=(
                    'normalizeê°€ Trueì¼ ê²½ìš°, í™•ë¥ ì´ ì‘ì€ í† í”½ì€ í•„í„°ë§ë©ë‹ˆë‹¤. '
                    'normalizeê°€ Falseì¼ ê²½ìš°, ì¸ìˆ˜ê°€ ì‘ì€ íŒ©í„°ê°€ í•„í„°ë§ë©ë‹ˆë‹¤. '
                    'Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ 1e-8 ê°’ì´ ì‚¬ìš©ë˜ì–´ 0ì„ ë°©ì§€í•©ë‹ˆë‹¤.'
                )
            ),
            'w_max_iter': st.number_input(
                'W ìµœëŒ€ ë°˜ë³µ',
                min_value=1,
                value=200,
                help='ê° ë°°ì¹˜ì—ì„œ Wë¥¼ í•™ìŠµí•  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'w_stop_condition': st.number_input(
                'W ì •ì§€ ì¡°ê±´',
                min_value=0.0,
                value=0.0001,
                help='ì˜¤ì°¨ ì°¨ì´ê°€ ì´ ê°’ë³´ë‹¤ ì‘ì•„ì§€ë©´ í˜„ì¬ ë°°ì¹˜ì˜ W í•™ìŠµì„ ì¤‘ì§€í•©ë‹ˆë‹¤.'
            ),
            'h_max_iter': st.number_input(
                'H ìµœëŒ€ ë°˜ë³µ',
                min_value=1,
                value=50,
                help='ê° ë°°ì¹˜ì—ì„œ Hë¥¼ í•™ìŠµí•  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'h_stop_condition': st.number_input(
                'H ì •ì§€ ì¡°ê±´',
                min_value=0.0,
                value=0.001,
                help='ì˜¤ì°¨ ì°¨ì´ê°€ ì´ ê°’ë³´ë‹¤ ì‘ì•„ì§€ë©´ í˜„ì¬ ë°°ì¹˜ì˜ H í•™ìŠµì„ ì¤‘ì§€í•©ë‹ˆë‹¤.'
            ),
            'eval_every': st.number_input(
                'í‰ê°€ ì£¼ê¸°',
                min_value=1,
                value=10,
                help='v - Whì˜ l2 ë…¸ë¦„ì„ ê³„ì‚°í•  ë°°ì¹˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'normalize': st.selectbox(
                'ì •ê·œí™”',
                (True, False, None),
                help='ê²°ê³¼ë¥¼ ì •ê·œí™”í• ì§€ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.'
            ),
            'submit': st.form_submit_button('ì ìš©')
        }

# ëª¨ë¸ ì •ë³´ ì‚¬ì „ ì •ì˜
MODELS = {
    'Latent Dirichlet Allocation': {
        'options': lda_options,
        'class': LdaModel,
        'help': 'https://radimrehurek.com/gensim/models/ldamodel.html'
    },
    'Non-Negative Matrix Factorization': {
        'options': nmf_options,
        'class': Nmf,
        'help': 'https://radimrehurek.com/gensim/models/nmf.html'
    }
}

# Matplotlibì—ì„œ ì‚¬ìš©í•  ìƒ‰ìƒ ëª©ë¡ ìƒì„±
COLORS = [color for color in mcolors.XKCD_COLORS.values()]

# íŒŒì¼ ì—…ë¡œë“œ í›„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_text(file):
    if file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        # docx íŒŒì¼ ì²˜ë¦¬
        try:
            doc = Document(file)
            return '\n'.join([para.text for para in doc.paragraphs])
        except Exception as e:
            st.error(f"docx íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file.name}\nì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            return ""
    elif file.type == "application/pdf":
        # pdf íŒŒì¼ ì²˜ë¦¬
        try:
            reader = PyPDF2.PdfReader(file)
            text = ''
            if reader.is_encrypted:
                try:
                    reader.decrypt('')
                except Exception as e:
                    st.warning(f"ì•”í˜¸í™”ëœ PDF íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file.name}")
                    return ""
            for page in reader.pages:
                extracted_text = page.extract_text()
                if extracted_text:
                    text += extracted_text + '\n'
            return text
        except Exception as e:
            st.error(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file.name}\nì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            return ""
    elif file.type == "text/plain":
        # txt íŒŒì¼ ì²˜ë¦¬
        try:
            return file.getvalue().decode("utf-8")
        except UnicodeDecodeError:
            try:
                return file.getvalue().decode("latin-1")
            except UnicodeDecodeError as e:
                st.error(f"txt íŒŒì¼ ì¸ì½”ë”©ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file.name}\nì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
                return ""
    else:
        st.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file.name}")
        return ""

# ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
@st.cache_data(show_spinner=False, ttl=600)
def create_texts_df(uploaded_files):
    texts = []
    for uploaded_file in uploaded_files:
        text = extract_text(uploaded_file)
        if text:
            # ë‹¨ë½ ë‹¨ìœ„ë¡œ ë¶„í• 
            paragraphs = [para.strip() for para in text.split('\n') if para.strip()]
            for para in paragraphs:
                texts.append({
                    'íŒŒì¼ëª…': uploaded_file.name,
                    'ë‹¨ë½': para
                })
    return pd.DataFrame(texts)

# ë¹…ê·¸ë¨(2-ê·¸ë¨)ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
@st.cache_data(show_spinner=False, ttl=600)
def create_bigrams(docs):
    bigram_phrases = Phrases(docs, min_count=5, threshold=100)
    bigram_phraser = Phraser(bigram_phrases)
    docs = [bigram_phraser[doc] for doc in docs]
    return docs

# íŠ¸ë¼ì´ê·¸ë¨(3-ê·¸ë¨)ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
@st.cache_data(show_spinner=False, ttl=600)
def create_trigrams(docs):
    bigram_phrases = Phrases(docs, min_count=5, threshold=100)
    bigram_phraser = Phraser(bigram_phrases)
    trigram_phrases = Phrases((bigram_phraser[doc] for doc in docs), min_count=5, threshold=100)
    trigram_phraser = Phraser(trigram_phrases)
    docs = [trigram_phraser[bigram_phraser[doc]] for doc in docs]
    return docs

# ë³µí•©ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ n-ê·¸ë¨ ëª¨ë¸ ì„¤ì •
def build_ngram_model(docs, min_count=5, threshold=100):
    bigram = Phrases(docs, min_count=min_count, threshold=threshold)
    return Phraser(bigram)

lemmatizer = WordNetLemmatizer()

def replace_keywords(docs, replacements=REPLACEMENTS):
    """
    ë¬¸ì„œ ë‚´ì˜ íŠ¹ì • í‚¤ì›Œë“œë¥¼ í‘œì¤€í™”ëœ í˜•íƒœë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.  
    Parameters:
        docs (list of list of str): í† í°í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸.
        replacements (dict): ëŒ€ì²´í•  í‚¤ì›Œë“œ ì‚¬ì „.  
    Returns:
        list of list of str: ëŒ€ì²´ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸.
    """
    new_docs = []
    for doc in docs:
        new_doc = []
        skip = False
        i = 0
        while i < len(doc):
            if skip:
                skip = False
                i += 1
                continue
            # ìµœëŒ€ í‚¤ì›Œë“œ ê¸¸ì´ì— ë§ì¶° ìŠ¬ë¼ì´ì‹± (ì—¬ê¸°ì„œëŠ” ìµœëŒ€ 2ë‹¨ì–´)
            replaced = False
            for phrase, replacement in replacements.items():
                phrase_tokens = phrase.split()
                phrase_length = len(phrase_tokens)
                if i + phrase_length <= len(doc) and doc[i:i+phrase_length] == phrase_tokens:
                    new_doc.append(replacement)
                    skip = True
                    replaced = True
                    break
            if not replaced:
                new_doc.append(doc[i])
            i += 1
        new_docs.append(new_doc)
    return new_docs

# ë¬¸ì„œì—ì„œ ë¶ˆí•„ìš”í•œ ìš”ì†Œë¥¼ ì œê±°í•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
@st.cache_data(show_spinner=False, ttl=600)
def denoise_docs(texts_df: pd.DataFrame, text_column: str):
    texts = texts_df[text_column].values.tolist()
    preprocessed_texts = []
    for text in texts:
        # ì´ë©”ì¼, ë©˜ì…˜, í•´ì‹œíƒœê·¸, URL ì œê±°
        remove_regex = regex.compile(f'({EMAIL_REGEX_STR}|{MENTION_REGEX_STR}|{HASHTAG_REGEX_STR}|{URL_REGEX_STR})')
        text = regex.sub(remove_regex, '', text)
        preprocessed_texts.append(text)

    additional_stopwords = {'he', 'she', 'it', 'they', 'we', 'us'}
    all_stopwords = set(stopwords.words('english')).union(additional_stopwords)

    # ë¶ˆìš©ì–´ ì œê±° ë° í‘œì œì–´ ì¶”ì¶œ
    docs = [
        [lemmatizer.lemmatize(w) for w in simple_preprocess(doc, deacc=True) if w not in all_stopwords]
        for doc in preprocessed_texts
    ]

    # ë¹…ê·¸ë¨ ìƒì„±
    bigram_model = build_ngram_model(docs)
    docs = [bigram_model[doc] for doc in docs]

    # íŠ¹ì • ë¹…ê·¸ë¨ì„ ë‹¨ì¼ ë‹¨ì–´ë¡œ ë³€í™˜ ëŒ€ì²´ í•¨ìˆ˜ ì ìš©
    docs = replace_keywords(docs)

    return docs

# ë¬¸ì„œ ìƒì„± í•¨ìˆ˜ë¡œ, ì „ì²˜ë¦¬ ë° n-ê·¸ë¨ ìƒì„±ì„ í¬í•¨
@st.cache_data(show_spinner=False, ttl=600)
def generate_docs(texts_df: pd.DataFrame, text_column: str, ngrams: str = None):
    docs = denoise_docs(texts_df, text_column)
    if ngrams == 'bigrams':
        docs = create_bigrams(docs)
    elif ngrams == 'trigrams':
        docs = create_trigrams(docs)
    return docs

# ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
@st.cache_data(show_spinner=False, ttl=600)
def generate_wordcloud(docs, collocations: bool = False, width=1400, height=1200):
    wordcloud_text = (' '.join(' '.join(doc) for doc in docs))
    wordcloud = WordCloud(
        font_path=WORDCLOUD_FONT_PATH,
        width=width,
        height=height,
        background_color='white',
        collocations=collocations
    ).generate(wordcloud_text)
    return wordcloud

# í›ˆë ¨ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜
@st.cache_data(show_spinner=False, ttl=600)
def prepare_training_data(docs):
    id2word = corpora.Dictionary(docs)
    corpus = [id2word.doc2bow(doc) for doc in docs]
    return id2word, corpus

# ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” í•¨ìˆ˜
def train_model(docs, base_model, **kwargs):
    id2word, corpus = prepare_training_data(docs)
    model = base_model(corpus=corpus, id2word=id2word, **kwargs)
    return id2word, corpus, model

# í¼í”Œë ‰ì„œí‹°ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
def calculate_perplexity(model, corpus):
    return np.exp2(-model.log_perplexity(corpus))

# ì½”íˆëŸ°ìŠ¤ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
def calculate_coherence(model, corpus, coherence):
    coherence_model = CoherenceModel(model=model, corpus=corpus, coherence=coherence)
    return coherence_model.get_coherence()

# í¼í”Œë ‰ì„œí‹° ì„¹ì…˜ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def perplexity_section(model, corpus):
    with st.spinner('í¼í”Œë ‰ì„œí‹° ê³„ì‚° ì¤‘...'):
        perplexity = calculate_perplexity(model, corpus)
    key = 'previous_perplexity'
    if key in st.session_state:
        delta = f'{perplexity - st.session_state[key]:.4}'
    else:
        delta = None
    st.metric(
        label='í¼í”Œë ‰ì„œí‹°',
        value=f'{perplexity:.4f}',
        delta=delta,
        delta_color='inverse'
    )
    st.markdown('ì°¸ê³ : https://en.wikipedia.org/wiki/Perplexity')
    st.latex(r'Perplexity = \exp\left(-\frac{\sum_d \log(p(w_d|\Phi, \alpha))}{N}\right)')
    st.session_state[key] = perplexity

# ì½”íˆëŸ°ìŠ¤ ì„¹ì…˜ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def coherence_section(model, corpus):
    with st.spinner('ì½”íˆëŸ°ìŠ¤ ì ìˆ˜ ê³„ì‚° ì¤‘...'):
        coherence = calculate_coherence(model, corpus, 'u_mass')
    key = 'previous_coherence_model_value'
    if key in st.session_state:
        delta = f'{coherence - st.session_state[key]:.4f}'
    else:
        delta = None
    st.metric(
        label='ì½”íˆëŸ°ìŠ¤ ì ìˆ˜',
        value=f'{coherence:.4f}',
        delta=delta
    )
    st.session_state[key] = coherence
    st.markdown('ì°¸ê³ : http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf')
    st.latex(
        r'C_{UMass} = \frac{2}{N \cdot (N - 1)}\sum_{i=2}^N\sum_{j=1}^{i-1}\log\frac{P(w_i, w_j) + \epsilon}{P(w_j)}'
    )

# ë©”íŠ¸ë¦­ ì„¹ì…˜ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def metrics_section(model, corpus):
    coherence_section(model, corpus)
    if hasattr(model, 'log_perplexity'):
        perplexity_section(model, corpus)

# ë°°ê²½ìƒ‰ì— ë”°ë¼ í…ìŠ¤íŠ¸ ìƒ‰ìƒì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
@st.cache_data(show_spinner=False, ttl=600)
def white_or_black_text(background_color):
    # ë°°ê²½ìƒ‰ì— ë”°ë¼ ê²€ì • ë˜ëŠ” í°ìƒ‰ í…ìŠ¤íŠ¸ ì„ íƒ
    red = int(background_color[1:3], 16)
    green = int(background_color[3:5], 16)
    blue = int(background_color[5:], 16)
    return 'black' if (red * 0.299 + green * 0.587 + blue * 0.114) > 186 else 'white'

#####################
### ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ###
#####################

# Streamlit ì•± ì‹¤í–‰
def main():
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title='í† í”½ëª¨ë¸ë§',
        page_icon='./data/favicon.png',
        layout='wide'
    )

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'model_kwargs' not in st.session_state:
        st.session_state['model_kwargs'] = {}
    if 'ngrams' not in st.session_state:
        st.session_state['ngrams'] = None
    if 'collocations' not in st.session_state:
        st.session_state['collocations'] = False
    if 'highlight_probability_minimum' not in st.session_state:
        st.session_state['highlight_probability_minimum'] = DEFAULT_HIGHLIGHT_PROBABILITY_MINIMUM
    if 'colors' not in st.session_state:
        st.session_state['colors'] = []

    # ì‚¬ì´ë“œë°”ì˜ ì „ì²˜ë¦¬ ì˜µì…˜ í¼
    preprocessing_options = st.sidebar.form('preprocessing-options')
    with preprocessing_options:
        st.header('ì „ì²˜ë¦¬ ì˜µì…˜')
        st.session_state.ngrams = st.selectbox(
            'N-ê·¸ë¨',
            [None, 'bigrams', 'trigrams'],
            help='ë‹¨ì–´ì˜ ê²°í•© ì •ë„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì˜ˆ: ë°”ì´ê·¸ë¨, íŠ¸ë¼ì´ê·¸ë¨'
        )
        preprocessing_submit = st.form_submit_button('ì „ì²˜ë¦¬ ì‹¤í–‰')

    # ì‚¬ì´ë“œë°”ì˜ ì‹œê°í™” ì˜µì…˜ í¼
    visualization_options = st.sidebar.form('visualization-options')
    with visualization_options:
        st.header('ì‹œê°í™” ì˜µì…˜')
        st.session_state.collocations = st.checkbox(
            'ì›Œë“œí´ë¼ìš°ë“œ ì½œë¡œì¼€ì´ì…˜ í™œì„±í™”',
            help='ì›Œë“œí´ë¼ìš°ë“œì—ì„œ êµ¬ë¬¸ì„ í‘œì‹œí•  ìˆ˜ ìˆë„ë¡ ì½œë¡œì¼€ì´ì…˜ì„ í™œì„±í™”í•©ë‹ˆë‹¤.'
        )
        st.session_state.highlight_probability_minimum = st.select_slider(
            'í•˜ì´ë¼ì´íŠ¸ í™•ë¥  ìµœì†Œê°’',
            options=[10 ** exponent for exponent in range(-10, 1)],
            value=DEFAULT_HIGHLIGHT_PROBABILITY_MINIMUM,
            help='_í† í”½ í•˜ì´ë¼ì´íŠ¸ ë¬¸ì¥_ ì‹œê°í™”ì—ì„œ ë‹¨ì–´ë¥¼ ìƒ‰ìƒìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸í•˜ê¸° ìœ„í•œ ìµœì†Œ í† í”½ í™•ë¥ ì„ ì„¤ì •í•©ë‹ˆë‹¤.'
        )
        visualization_submit = st.form_submit_button('ì ìš©')

    # ì‚¬ì´ë“œë°”ì˜ ëª¨ë¸ ì„ íƒ
    model_selection = st.sidebar.selectbox(
        'ëª¨ë¸ ì„ íƒ',
        list(MODELS.keys()),
        help='í† í”½ ëª¨ë¸ë§ì— ì‚¬ìš©í•  ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•©ë‹ˆë‹¤.'
    )

    # ì‚¬ì´ë“œë°”ì˜ ëª¨ë¸ ì˜µì…˜ í¼
    model_options = MODELS[model_selection]['options']()

    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì œëª© ë° ì„¤ëª…
    st.title('Topic Modeling')
    st.subheader('í† í”½ëª¨ë¸ë§ì´ë€?')
    st.markdown(
        """
ë¬¸ì„œë“¤ì˜ ì§‘í•©ì—ì„œ íŠ¹ì •í•œ ì£¼ì œë¥¼ ì°¾ì•„ë‚´ê¸° ìœ„í•œ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ (NLP)ë¡œ,  
íŠ¹ì • ì£¼ì œì— ê´€í•œ ë¬¸ì„œì—ì„œëŠ” íŠ¹ì • ë‹¨ì–´ê°€ ìì£¼ ë“±ì¥í•  ê²ƒì´ë¼ëŠ” ì§ê´€ì—ì„œ ì‹œì‘ëœ ê¸°ìˆ ì…ë‹ˆë‹¤.  
ì˜ˆë¥¼ ë“¤ì–´ì„œ íŠ¹ì •í•œ ë¬¸ì„œì˜ ì£¼ì œê°€ ìŒì‹ì´ë¼ë©´, ìŒì‹ì˜ ì¢…ë¥˜, ìŒì‹ì˜ ì¬ë£Œ ë“±ì„ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë¬¸ì„œì— ë¹„í•´ ë§ì´ ë“±ì¥í•œë‹¤ê³  ë³´ê³ ,  
ê·¸ íŠ¹ì •í•œ ì£¼ì œë¥¼ ì°¾ì•„ë‚´ê¸° ìœ„í•œ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.
"""
    )
    st.markdown(
        """
í† í”½ ëª¨ë¸ë§ì€ íŠ¹íˆ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ê¸°ë²• ì¤‘ì—ì„œë„ ê°€ì¥ ë§ì´ í™œìš©ë˜ëŠ” ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.  
í† í”½ ëª¨ë¸ë§ì€ ë‹¤ì‹œ ë‘ ê°€ì§€ì˜ ë°©ë²•ìœ¼ë¡œ êµ¬ë¶„ë˜ëŠ”ë°, í•˜ë‚˜ëŠ” ì ì¬ ì˜ë¯¸ ë¶„ì„(LSA; Latent Semantic Analysis)ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹(LDA; Latent Dirichlet Allocation)ì…ë‹ˆë‹¤.
"""
    )

    # ì¶”ê°€ ì„¸ë¶€ ì •ë³´ í™•ì¥
    with st.expander('ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹'):
        st.image('data/LDA.png')
        st.markdown(
            'LDA(Latent Dirichlet Allocation)ëŠ” í™•ë¥ ì  ê·¸ë˜í”½ ëª¨ë¸ë¡œ, ë¬¸ì„œ ì§‘í•© ë‚´ì— ìˆ¨ê²¨ì§„ í† í”½ì„ ë°œê²¬í•˜ê¸° ìœ„í•´ ê° ë¬¸ì„œë¥¼ ì—¬ëŸ¬ í† í”½ì˜ í˜¼í•©ìœ¼ë¡œ ë³´ê³ , ê° í† í”½ì„ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ë¡œ ì •ì˜í•©ë‹ˆë‹¤. LDAëŠ” ë¬¸ì„œ ë‚´ ë‹¨ì–´ë“¤ì´ íŠ¹ì • í† í”½ì—ì„œ ìƒì„±ë  í™•ë¥ ì„ ì¶”ì •í•˜ì—¬, ë¬¸ì„œì™€ í† í”½ ê°„ì˜ ê´€ê³„ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¬¸ì„œ ì§‘í•©ì˜ ì ì¬ì  êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³ , ì£¼ì œë³„ë¡œ ë¬¸ì„œë¥¼ ë¶„ë¥˜í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë° ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.'
        )

    with st.expander('ë¹„ìŒìˆ˜ í–‰ë ¬ ì¸ìˆ˜ë¶„í•´ ê¸°ë²•'):
        st.image('data/NMF.png')
        st.markdown(
            'NMF(Non-negative Matrix Factorization)ëŠ” ë¹„ìŒìˆ˜ í–‰ë ¬ ì¸ìˆ˜ë¶„í•´ ê¸°ë²•ìœ¼ë¡œ, ë‹¨ì–´-ë¬¸ì„œ í–‰ë ¬ì„ ë‘ ê°œì˜ ì €ì°¨ì› ë¹„ìŒìˆ˜ í–‰ë ¬ë¡œ ë¶„í•´í•˜ì—¬ í† í”½ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. NMFëŠ” ì›ë˜ í–‰ë ¬ì„ ë‹¨ì–´-í† í”½ í–‰ë ¬ê³¼ í† í”½-ë¬¸ì„œ í–‰ë ¬ë¡œ ë¶„í•´í•¨ìœ¼ë¡œì¨, ê° í† í”½ì„ ë‹¨ì–´ë“¤ì˜ ê°€ì¤‘ì¹˜ ì¡°í•©ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê³„ì‚°ì´ ë¹„êµì  ê°„ë‹¨í•˜ê³  ë¹ ë¥´ë©°, ë¹„ìŒìˆ˜ ì œì•½ìœ¼ë¡œ ì¸í•´ ê²°ê³¼ í•´ì„ì´ ìš©ì´í•˜ì—¬ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹¤ë§Œ, ì´ˆê¸°í™”ì— ë¯¼ê°í•˜ê³  í™•ë¥ ì  í•´ì„ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.'
        )

    # íŒŒì¼ ì—…ë¡œë” ì¶”ê°€ (ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œë¡œ ë³€ê²½)
    st.subheader('íŒŒì¼ ì—…ë¡œë“œ')
    uploaded_file = st.file_uploader(
        "docx, pdf, txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['docx', 'pdf', 'txt'],
        accept_multiple_files=False  # í•˜ë‚˜ì˜ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
    )

    if not uploaded_file:
        st.warning('ì²˜ë¦¬í•  íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.')
        st.stop()

    # íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ì ì œê±° ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±°
    file_root, file_ext = os.path.splitext(uploaded_file.name)
    file_root = re.sub(r'[^A-Za-z0-9_-]', '_', file_root)  # íŠ¹ìˆ˜ ë¬¸ì ì œê±°

    # ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë°ì´í„°í”„ë ˆì„ ìƒì„±
    texts_df = create_texts_df([uploaded_file])  # ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ í•¨ìˆ˜ì— ì „ë‹¬

    if texts_df.empty:
        st.error('ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        st.stop()

    # ëª¨ë¸ í•™ìŠµ ë° ì„¤ì •
    if model_selection not in MODELS:
        st.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.stop()

    # ëª¨ë¸ ì˜µì…˜ ì ìš©
    if model_options.get('submit'):
        # 'submit' í‚¤ë¥¼ ì œì™¸í•˜ê³  ë‹¤ë¥¸ ì„¤ì •ë§Œ ì €ì¥
        st.session_state['model_kwargs'] = {k: v for k, v in model_options.items() if k != 'submit'}
        st.success(f"{model_selection} ì˜µì…˜ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ í•™ìŠµ ìë™ ì‹¤í–‰
        with st.spinner('ëª¨ë¸ í•™ìŠµ ì¤‘...'):
            docs = generate_docs(texts_df, 'ë‹¨ë½', ngrams=st.session_state.ngrams)
            id2word, corpus, model = train_model(
                docs,
                MODELS[model_selection]['class'],
                **st.session_state.get('model_kwargs', {})
            )
        current_model = model  # ëª¨ë¸ì„ ë³€ìˆ˜ë¡œ ì €ì¥

        # ëª¨ë¸ì˜ ì‹¤ì œ í† í”½ ìˆ˜ì— ë§ê²Œ colors ì´ˆê¸°í™” ë° ì €ì¥
        if model.num_topics <= len(COLORS):
            st.session_state.colors = random.sample(COLORS, k=model.num_topics)
        else:
            # COLORS ë¦¬ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš°, ìƒ‰ìƒì„ ë°˜ë³µí•´ì„œ ì‚¬ìš©
            st.session_state.colors = [COLORS[i % len(COLORS)] for i in range(model.num_topics)]

        st.session_state.id2word = id2word
        st.session_state.corpus = corpus

        st.success('ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!')

    else:
        st.info('ëª¨ë¸ ì˜µì…˜ì„ ì„¤ì •í•˜ê³  "ì ìš©" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.')
        st.stop()

    # íƒ­ ì„¤ì •
    tabs = st.tabs(["ê°œë³„ íŒŒì¼ ë¶„ì„", "pyLDAvis ì‹œê°í™”"])

    with tabs[0]:
        st.header('ê°œë³„ íŒŒì¼ ë¶„ì„')

        # ë‹¨ì¼ íŒŒì¼ì´ë¯€ë¡œ ë£¨í”„ ì œê±°
        st.subheader(f'íŒŒì¼: {file_root}{file_ext}')
        file_df = texts_df[texts_df['íŒŒì¼ëª…'] == uploaded_file.name]
        docs = generate_docs(file_df, 'ë‹¨ë½', ngrams=st.session_state.ngrams)

        # 1) ì…ë ¥ ë¬¸ì„œ ìƒ˜í”Œ
        with st.expander('ì…ë ¥ ë¬¸ì„œ ìƒ˜í”Œ'):
            sample_size = min(5, len(docs))
            if sample_size > 0:
                sample_texts = random.sample(docs, sample_size)
                for idx, doc in enumerate(sample_texts):
                    st.markdown(f'**ìƒ˜í”Œ {idx + 1}**: _{" ".join(doc)}_')
            else:
                st.write("ìƒ˜í”Œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 2) ì›Œë“œ í´ë¼ìš°ë“œ
        with st.expander('ì›Œë“œí´ë¼ìš°ë“œ'):
            wc = generate_wordcloud(docs, collocations=st.session_state.collocations)
            st.image(wc.to_image(), caption='ì›Œë“œí´ë¼ìš°ë“œ', use_column_width=True)

            # Download ë²„íŠ¼ ì¶”ê°€
            buf = BytesIO()
            wc.to_image().save(buf, format='PNG')
            byte_im = buf.getvalue()
            st.download_button(
                label="ì›Œë“œí´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ",
                data=byte_im,
                file_name=f'wordcloud_{file_root}.png',  # í™•ì¥ì ì œê±°ëœ íŒŒì¼ ì´ë¦„ ì‚¬ìš©
                mime='image/png'
            )

        # 3) ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜
        with st.expander('ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜'):
            all_words = [word for doc in docs for word in doc]
            word_freq = pd.Series(all_words).value_counts().reset_index()
            word_freq.columns = ['í‚¤ì›Œë“œ', 'ë¹ˆë„ìˆ˜']
            top_20_words = word_freq.head(20)

            # Plotly Expressë¥¼ ì‚¬ìš©í•œ ì¸í„°ë™í‹°ë¸Œ ë°” ì°¨íŠ¸ ìƒì„±
            fig = px.bar(
                top_20_words,
                x='í‚¤ì›Œë“œ',
                y='ë¹ˆë„ìˆ˜',
                labels={'í‚¤ì›Œë“œ': 'í‚¤ì›Œë“œ', 'ë¹ˆë„ìˆ˜': 'ë¹ˆë„ìˆ˜'},
                title='ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜',
                text='ë¹ˆë„ìˆ˜',
                color='ë¹ˆë„ìˆ˜',
                color_continuous_scale='Blues'
            )

            # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸ (ì˜µì…˜)
            fig.update_layout(
                xaxis_title='í‚¤ì›Œë“œ',
                yaxis_title='ë¹ˆë„ìˆ˜',
                bargap=0.2,  # ë§‰ëŒ€ ì‚¬ì´ ê°„ê²©
                template='plotly_white'  # í°ìƒ‰ ë°°ê²½ í…œí”Œë¦¿
            )

            # í…ìŠ¤íŠ¸ ë ˆì´ë¸” í¬ë§· ì„¤ì •
            fig.update_traces(texttemplate='%{text}', textposition='outside')

            # Plotly ë°” ì°¨íŠ¸ë¥¼ Streamlitì— í‘œì‹œ
            st.plotly_chart(fig, use_container_width=True)

            # 2íšŒ ì´ìƒ ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œ ëª©ë¡ ë° ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
            keywords_twice = word_freq[word_freq['ë¹ˆë„ìˆ˜'] >= 2]
            st.subheader('2íšŒ ì´ìƒ ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œ ëª©ë¡')

            # ë°ì´í„°í”„ë ˆì„ì„ ì–‘ ì˜†ìœ¼ë¡œ ë„“ê²Œ í‘œì‹œ
            st.dataframe(keywords_twice, use_container_width=True)

            # ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ (ìˆ˜ì •ëœ ë¶€ë¶„)
            towrite = BytesIO()
            keywords_twice.to_excel(towrite, index=False, engine='openpyxl')
            towrite.seek(0)
            st.download_button(
                label="ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ",
                data=towrite,
                file_name=f'keywords_twice_{file_root}.xlsx',  # í™•ì¥ì ì œê±°ëœ íŒŒì¼ ì´ë¦„ ì‚¬ìš©
                mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )

        # 4) í† í”½ ë‹¨ì–´ ê°€ì¤‘ì¹˜ ìš”ì•½
        with st.expander('í† í”½ ë‹¨ì–´ ê°€ì¤‘ì¹˜ ìš”ì•½'):
            # í† í”½ ë‹¨ì–´ ê°€ì¤‘ì¹˜ ìš”ì•½ ì„¹ì…˜
            num_topics = st.session_state['model_kwargs'].get('num_topics', 9)
            topics = current_model.show_topics(
                formatted=False,
                num_words=50,
                num_topics=num_topics,
                log=False
            )
            topic_summaries = {}
            for topic in topics:
                topic_index = topic[0]
                topic_word_weights = topic[1]
                # ìƒìœ„ 10ê°œì˜ ë‹¨ì–´ì™€ ê°€ì¤‘ì¹˜ë¥¼ ìš”ì•½
                topic_summaries[topic_index] = ' + '.join(
                    f'{weight:.3f} * {word}' for word, weight in topic_word_weights[:10]
                )
            for topic_index, topic_summary in topic_summaries.items():
                st.markdown(f'**í† í”½ {topic_index}**: _{topic_summary}_')

        # 5) í† í”½ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ
        with st.expander('í† í”½ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ'):
            colors = st.session_state.colors  # session_stateì—ì„œ colors ì°¸ì¡°

            def make_color_func(color):
                def color_func(word, font_size, position, orientation, random_state=None, **kwargs):
                    return color
                return color_func

            cols = st.columns(3)  # 3ê°œì˜ ì—´ë¡œ ë‚˜ëˆ„ê¸°
            for index, topic in enumerate(topics):
                # ìƒ‰ìƒì„ ì„ íƒ
                color = colors[index % len(colors)]

                # color_funcë¥¼ ìƒì„±
                color_func_custom = make_color_func(color)

                # WordCloud ê°ì²´ ìƒì„±
                wc = WordCloud(
                    font_path=WORDCLOUD_FONT_PATH,
                    width=1400,  # ê³ í•´ìƒë„
                    height=1200,  # ê³ í•´ìƒë„
                    background_color='white',
                    collocations=st.session_state.collocations,
                    prefer_horizontal=1.0,
                    color_func=color_func_custom  # ìˆ˜ì •ëœ ë¶€ë¶„
                )

                # í† í”½ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                topic_freq = dict(topic[1])

                # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
                wc.generate_from_frequencies(topic_freq)

                # ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ë¥¼ ì—´ì— í‘œì‹œ
                with cols[index % 3]:
                    st.image(wc.to_image(), caption=f'í† í”½ #{index}', use_column_width=True)

                    # Download ë²„íŠ¼ ì¶”ê°€
                    buf = BytesIO()
                    wc.to_image().save(buf, format='PNG')
                    byte_im = buf.getvalue()
                    st.download_button(
                        label=f"í† í”½ {index} ì›Œë“œí´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ",
                        data=byte_im,
                        file_name=f'topic_{index}_wordcloud_{file_root}.png',  # í™•ì¥ì ì œê±°ëœ íŒŒì¼ ì´ë¦„ ì‚¬ìš©
                        mime='image/png'
                    )

        # 6) í† í”½ í•˜ì´ë¼ì´íŠ¸ ë¬¸ì¥
        with st.expander('í† í”½ í•˜ì´ë¼ì´íŠ¸ ë¬¸ì¥'):
            for idx, doc in enumerate(docs[:10]):  # ìƒ˜í”Œ í¬ê¸° ì¡°ì •
                html_elements = []
                for token in doc:
                    if st.session_state.id2word.token2id.get(token) is None:
                        # ë‹¨ì–´ê°€ ì‚¬ì „ì— ì—†ìœ¼ë©´ ì·¨ì†Œì„  ì ìš©
                        html_elements.append(f'<span style="text-decoration:line-through;">{token}</span>')
                    else:
                        term_topics = current_model.get_term_topics(token, minimum_probability=0)
                        topic_probabilities = [term_topic[1] for term_topic in term_topics]
                        max_topic_probability = max(topic_probabilities) if topic_probabilities else 0
                        if max_topic_probability < st.session_state.highlight_probability_minimum:
                            # í™•ë¥ ì´ ìµœì†Œê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¼ë°˜ í…ìŠ¤íŠ¸
                            html_elements.append(token)
                        else:
                            # ìµœëŒ€ í™•ë¥ ì„ ê°€ì§„ í† í”½ì˜ ìƒ‰ìƒìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸
                            max_topic_index = topic_probabilities.index(max_topic_probability)
                            max_topic = term_topics[max_topic_index]
                            
                            # max_topic[0]ì´ colors ë¦¬ìŠ¤íŠ¸ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í™•ì¸
                            if max_topic[0] < len(st.session_state.colors):
                                background_color = st.session_state.colors[max_topic[0]]
                            else:
                                background_color = 'grey'  # ê¸°ë³¸ ìƒ‰ìƒ ë˜ëŠ” ëŒ€ì²´ ìƒ‰ìƒ

                            color = white_or_black_text(background_color)
                            html_elements.append(
                                f'<span style="background-color: {background_color}; color: {color}; opacity: 0.5;">{token}</span>'
                            )
                st.markdown(f'ë¬¸ì„œ #{idx + 1}: {" ".join(html_elements)}', unsafe_allow_html=True)

        # 7) ë©”íŠ¸ë¦­
        with st.expander('ë©”íŠ¸ë¦­'):
            metrics_section(current_model, corpus)

    with tabs[1]:
        st.header('pyLDAvis ì‹œê°í™”')
        try:
            py_lda_vis_data = pyLDAvis.gensim_models.prepare(
                current_model,
                corpus,
                id2word
            )
            py_lda_vis_html = pyLDAvis.prepared_data_to_html(py_lda_vis_data)

            # Display pyLDAvis in a larger iframe
            st.components.v1.html(py_lda_vis_html, width=1600, height=900, scrolling=True)

            # Download pyLDAvis HTML
            buffer = BytesIO()
            buffer.write(py_lda_vis_html.encode('utf-8'))
            buffer.seek(0)
            st.download_button(
                label="pyLDAvis HTML ë‹¤ìš´ë¡œë“œ",
                data=buffer,
                file_name=f'pyLDAvis_{file_root}.html',  # í•„ìš”ì— ë”°ë¼ file_root í¬í•¨
                mime='text/html'
            )

        except Exception as e:
            st.error(f"pyLDAvis ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == '__main__':
    main()
